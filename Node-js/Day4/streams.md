Node.js me Streams ka use large data ko chhote-chhote parts (chunks) me process karne ke liye hota hai â€” bina poora data memory me load kiye.
Ye fast aur memory-efficient hota hai.

ğŸ”¹ Types of Streams

Readable Stream â†’ Data read karne ke liye (ex: fs.createReadStream())

Writable Stream â†’ Data likhne ke liye (ex: fs.createWriteStream())

Duplex Stream â†’ Dono (read + write) kar sakta hai (ex: net.Socket)

Transform Stream â†’ Data ko modify karte hue pass karta hai (ex: zlib compression)

ğŸ”¹ Example (File Read + Write Stream)
const fs = require('fs');

const readStream = fs.createReadStream('input.txt');
const writeStream = fs.createWriteStream('output.txt');

// Pipe connects read â†’ write
readStream.pipe(writeStream);

console.log('Data copied successfully!');


ğŸ‘‰ Yahaan pipe() method data ko streaming way me ek file se dusri file me transfer karta hai.

ğŸ”¹ Why Streams?

Large file handle karne me fast aur memory-efficient

Non-blocking (async)

Used in file operations, network requests, and audio/video streaming.









treams Internally Kaise Kaam Karte Hain?

Node.js me Streams basically EventEmitter par based hote hain.
Matlab â€” ye events fire karte hain jaise:

data â†’ jab naya chunk aata hai

end â†’ jab poora data read ho jata hai

error â†’ jab koi error hoti hai

finish â†’ jab likhna complete hota hai

ğŸ”¹ Step-by-Step Working (Readable Stream Example)
Example Code:
const fs = require('fs');
const readStream = fs.createReadStream('file.txt');

readStream.on('data', (chunk) => {
  console.log('New chunk:', chunk.toString());
});

readStream.on('end', () => {
  console.log('File reading done!');
});

Internal Process ğŸ”

File open hoti hai:
fs.createReadStream() OS-level file handle open karta hai.

Buffer allocate hota hai:
Node ek internal buffer (default ~64KB) banata hai jisme file ka ek part load hota hai.

Chunk emit hota hai:
Jab buffer fill hota hai, stream ek data event emit karta hai aur tumhe ek chunk deta hai.

Flow Control:
Agar tum data fast process nahi kar pa rahe ho, Node stream ko pause kar deta hai automatically.
Jab tum ready ho jaate ho, wo fir se resume karta hai.

End event:
Jab pura file read ho jaata hai â†’ end event fire hota hai.

ğŸ”¹ Writable Stream Internally
const fs = require('fs');
const writeStream = fs.createWriteStream('output.txt');

writeStream.write('Hello Stream!');
writeStream.end();


Write request queue hoti hai
Node data ko ek internal queue me store karta hai.

OS ko write operation bheja jaata hai (asynchronously).

Jab likhna khatam hota hai â†’ drain aur finish events trigger hote hain.

ğŸ”¹ Pipe Working Internally
readStream.pipe(writeStream);


pipe() basically internally data event ko listen karta hai.

Jab bhi readStream me data event aata hai â†’ wo us chunk ko writeStream.write() me bhej deta hai.

Jab readStream ka end aata hai â†’ writeStream.end() call hota hai automatically.

So:
pipe() = connect karta hai source (Readable) ko destination (Writable) se without manual event handling.

ğŸ”¹ Summary Table
Stream Type	Example	Emits Events	Internal Role
Readable	fs.createReadStream()	data, end	Read data in chunks
Writable	fs.createWriteStream()	drain, finish	Write data in chunks
Duplex	net.Socket	Both	Read + Write
Transform	zlib.createGzip()	Both + transform	Modify data on the fly
ğŸ§  Simple Analogy:

Imagine ek pipeline:

Tap (Readable) â†’ paani chhote flow me deta hai (chunks)

Pipe (Transform) â†’ paani filter karta hai

Bucket (Writable) â†’ paani collect karta hai

Node.js streams bilkul isi tarah asynchronously data pass karte hain â€” bina saara data ek baar me load kiye.